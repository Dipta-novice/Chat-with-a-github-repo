{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a25e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import chromadb\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import subprocess\n",
    "from langchain_community.document_loaders import GitLoader\n",
    "#from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e1908a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url='https://github.com/Dipta-novice/Research-Paper-Sumarizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa186790",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name = repo_url.split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "115cdc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = f\"./cloned_repos/{repo_name}\"\n",
    "include_extensions = ('.py', '.md', '.ipynb', '.txt', '.json', '.yml', '.yaml')\n",
    "\n",
    "# Load files with GitLoader\n",
    "loader = GitLoader(\n",
    "    repo_path=repo_path,\n",
    "    branch='main',\n",
    "    file_filter=lambda file_path: any(file_path.endswith(ext) for ext in include_extensions)\n",
    ")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e1560f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Paper_Summarizer.py', 'file_path': 'Paper_Summarizer.py', 'file_name': 'Paper_Summarizer.py', 'file_type': '.py'}, page_content='from langchain_google_genai import ChatGoogleGenerativeAI\\r\\nfrom dotenv import load_dotenv\\r\\nimport os\\r\\nimport streamlit as st\\r\\nfrom langchain_core.prompts import PromptTemplate,load_prompt\\r\\n\\r\\nload_dotenv()\\r\\napi_key = os.getenv(\"GEMINI_API_KEY\")\\r\\n# Initialize Gemini model\\r\\nllm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=api_key)\\r\\n\\r\\nst.header(\\'Reasearch Tool\\')\\r\\n\\r\\npaper_input = st.selectbox( \"Select Research Paper Name\", [\"Attention Is All You Need\", \"BERT: Pre-training of Deep Bidirectional Transformers\", \"GPT-3: Language Models are Few-Shot Learners\", \"Diffusion Models Beat GANs on Image Synthesis\"] )\\r\\n\\r\\nstyle_input = st.selectbox( \"Select Explanation Style\", [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"] ) \\r\\n\\r\\nlength_input = st.selectbox( \"Select Explanation Length\", [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"] )\\r\\n\\r\\ntemplate = load_prompt(\\'template.json\\')\\r\\n\\r\\n\\r\\n\\r\\nif st.button(\\'Summarize\\'):\\r\\n    chain = template | llm\\r\\n    result = chain.invoke({\\r\\n        \\'paper_input\\':paper_input,\\r\\n        \\'style_input\\':style_input,\\r\\n        \\'length_input\\':length_input\\r\\n    })\\r\\n    st.write(result.content)'),\n",
       " Document(metadata={'source': 'promptgenerator.py', 'file_path': 'promptgenerator.py', 'file_name': 'promptgenerator.py', 'file_type': '.py'}, page_content='from langchain_core.prompts import PromptTemplate\\r\\n\\r\\n# template\\r\\ntemplate = PromptTemplate(\\r\\n    template=\"\"\"\\r\\nPlease summarize the research paper titled \"{paper_input}\" with the following specifications:\\r\\nExplanation Style: {style_input}  \\r\\nExplanation Length: {length_input}  \\r\\n1. Mathematical Details:  \\r\\n   - Include relevant mathematical equations if present in the paper.  \\r\\n   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.  \\r\\n2. Analogies:  \\r\\n   - Use relatable analogies to simplify complex ideas.  \\r\\nIf certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.  \\r\\nEnsure the summary is clear, accurate, and aligned with the provided style and length.\\r\\n\"\"\",\\r\\ninput_variables=[\\'paper_input\\', \\'style_input\\',\\'length_input\\'],\\r\\nvalidate_template=True\\r\\n)\\r\\n\\r\\ntemplate.save(\\'template.json\\')'),\n",
       " Document(metadata={'source': 'requirements.txt', 'file_path': 'requirements.txt', 'file_name': 'requirements.txt', 'file_type': '.txt'}, page_content='# LangChain Core\\r\\nlangchain\\r\\nlangchain-core\\r\\n\\r\\n# OpenAI Integration\\r\\nlangchain-openai\\r\\nopenai\\r\\n\\r\\n# Anthropic Integration\\r\\nlangchain-anthropic\\r\\n\\r\\n# Google Gemini (PaLM) Integration\\r\\nlangchain-google-genai\\r\\ngoogle-generativeai\\r\\n\\r\\n# Hugging Face Integration\\r\\nlangchain-huggingface\\r\\ntransformers\\r\\nhuggingface-hub\\r\\n\\r\\n# Environment Variable Management\\r\\npython-dotenv\\r\\n\\r\\n# Machine Learning Utilities\\r\\nnumpy\\r\\nscikit-learn\\r\\n\\r\\nstreamlit')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f58d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abdbee7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m31 packages\u001b[0m \u001b[2min 453ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to uninstall package at \u001b[36m.venv\\Lib\\site-packages\\tokenizers-0.21.1.dist-info\u001b[39m due to missing `RECORD` file. Installation may result in an incomplete environment.\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m1 package\u001b[0m \u001b[2min 0.62ms\u001b[0m\u001b[0m\n",
      "\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 55ms\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.21.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.20.3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db8561e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "!!!!!!!!!!!!megablocks not available, using torch.matmul instead\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"nomic-ai/nomic-embed-text-v1\",\n",
    "    model_kwargs={\"trust_remote_code\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a64c85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "# 3Ô∏è‚É£ Initialize ChromaDB persistent client\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_store\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"github_repo_chat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb0ede30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Process and add documents\n",
    "for doc in documents:\n",
    "    file_path = doc.metadata.get('source', 'unknown')\n",
    "    chunks = splitter.split_text(doc.page_content)\n",
    "\n",
    "    ids = [f\"{file_path}_chunk_{i}\" for i in range(len(chunks))]\n",
    "    metadatas = [{\"file\": file_path} for _ in chunks]\n",
    "\n",
    "    # Add chunks to the vector store ‚Äî embeddings auto-generated internally\n",
    "    embeddings = embeddings_model.embed_documents(chunks)\n",
    "\n",
    "    collection.add(\n",
    "    documents=chunks,\n",
    "    embeddings=embeddings,  # now actual vectors\n",
    "    metadatas=metadatas,\n",
    "    ids=ids\n",
    "    )\n",
    "\n",
    "# save to disk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef033bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chroma(query, top_k=5):\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfee85af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key='AIzaSyCGbuEtVoL9-b1K7O1Tn7qS94Vf9bzU9as')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30182658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Gemini Analysis:\n",
      "This GitHub repository is about creating a research paper summarization tool using LangChain and Google's Gemini (PaLM) model. The tool allows users to select a research paper, explanation style, and length, and then generates a summary based on these parameters. Here's a breakdown of the key aspects:\n",
      "\n",
      "**1. Core Functionality:**\n",
      "\n",
      "*   **Summarization:** The primary purpose is to summarize research papers. The code uses `langchain-google-genai` to interact with the Gemini model for text generation.\n",
      "\n",
      "*   **Customization:** Users can customize the summary through:\n",
      "    *   `paper_input`: Selecting a specific research paper from a predefined list (e.g., \"Attention Is All You Need\").\n",
      "    *   `style_input`: Choosing an explanation style (e.g., \"Beginner-Friendly\", \"Technical\").\n",
      "    *   `length_input`: Specifying the desired length of the summary (e.g., \"Short\", \"Medium\", \"Long\").\n",
      "\n",
      "**2. Key Technologies & Libraries:**\n",
      "\n",
      "*   **LangChain:**  The foundation of the tool.  The code imports modules from `langchain-core` and `langchain-google-genai`. LangChain is used for:\n",
      "    *   **Prompt Management:**  `langchain_core.prompts.PromptTemplate` defines the structure of the prompt sent to the LLM.  The `load_prompt` function is also used, suggesting that prompts can be loaded from external files.\n",
      "    *   **Chaining:** LangChain chains together the prompt template and the LLM (`chain = template | llm`) to create a complete summarization pipeline.\n",
      "    *   **LLM Integration:** `ChatGoogleGenerativeAI` from `langchain-google-genai` is used to interface with the Gemini model.\n",
      "\n",
      "*   **Google Gemini (PaLM):**  The large language model used for generating the summaries.  The code initializes the model with `llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=api_key)`. The `google-generativeai` package is implicitly used through the LangChain integration.\n",
      "\n",
      "*   **Streamlit:**  Provides the user interface for the tool.  The code uses Streamlit components like `st.header`, `st.selectbox`, and `st.button` to create the interactive elements.\n",
      "\n",
      "*   **`python-dotenv`:** Used for managing environment variables, specifically the Gemini API key.  `load_dotenv()` loads the API key from a `.env` file.\n",
      "\n",
      "**3. Code Walkthrough:**\n",
      "\n",
      "*   **Environment Setup:** Loads the Gemini API key from environment variables using `dotenv`.\n",
      "\n",
      "    ```python\n",
      "    from dotenv import load_dotenv\n",
      "    import os\n",
      "\n",
      "    load_dotenv()\n",
      "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
      "    ```\n",
      "\n",
      "*   **LLM Initialization:** Initializes the `ChatGoogleGenerativeAI` model.\n",
      "\n",
      "    ```python\n",
      "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=api_key)\n",
      "    ```\n",
      "\n",
      "*   **Streamlit UI:** Creates the UI elements using Streamlit.\n",
      "\n",
      "    ```python\n",
      "    import streamlit as st\n",
      "\n",
      "    st.header('Reasearch Tool')\n",
      "\n",
      "    paper_input = st.selectbox( \"Select Research Paper Name\", [\"Attention Is All You Need\", \"BERT: Pre-training of Deep Bidirectional Transformers\", \"GPT-3: Language Models are Few-Shot Learners\", \"Diffusion Models Beat GANs on Image Synthesis\"] )\n",
      "\n",
      "    style_input = st.selectbox( \"Select Explanation Style\", [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"] )\n",
      "\n",
      "    length_input = st.selectbox( \"Select Explanation Length\", [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"] )\n",
      "    ```\n",
      "\n",
      "*   **Prompt Template:** Defines the prompt sent to the LLM.  The prompt includes instructions on the desired style, length, and content of the summary. The prompt is initially loaded from `template.json` but then redefined in the code and saved back to `template.json`.\n",
      "\n",
      "    ```python\n",
      "    from langchain_core.prompts import PromptTemplate\n",
      "\n",
      "    template = PromptTemplate(\n",
      "        template=\"\"\"\n",
      "    Please summarize the research paper titled \"{paper_input}\" with the following specifications:\n",
      "    Explanation Style: {style_input}\n",
      "    Explanation Length: {length_input}\n",
      "    1. Mathematical Details:\n",
      "       - Include relevant mathematical equations if present in the paper.\n",
      "       - Explain the mathematical concepts using simple, intuitive code snippets where applicable.\n",
      "    2. Analogies:\n",
      "       - Use relatable analogies to simplify complex ideas.\n",
      "    If certain information is not available in the paper, respond with: \"Insufficient information available\" instead of guessing.\n",
      "    Ensure the summary is clear, accurate, and aligned with the provided style and length.\n",
      "    \"\"\",\n",
      "        input_variables=['paper_input', 'style_input','length_input'],\n",
      "        validate_template=True\n",
      "    )\n",
      "\n",
      "    template.save('template.json')\n",
      "    ```\n",
      "\n",
      "*   **Summarization Execution:** When the \"Summarize\" button is clicked, the code invokes the LangChain chain to generate the summary.\n",
      "\n",
      "    ```python\n",
      "    if st.button('Summarize'):\n",
      "        chain = template | llm\n",
      "        result = chain.invoke({\n",
      "            'paper_input':paper_input,\n",
      "            'style_input':style_input,\n",
      "            'length_input':length_input\n",
      "        })\n",
      "        st.write(result.content)\n",
      "    ```\n",
      "\n",
      "**4. Patterns and Implementation Details:**\n",
      "\n",
      "*   **Environment Variable Configuration:** The code uses `.env` files and `python-dotenv` to securely manage API keys.\n",
      "*   **Prompt Engineering:** The code demonstrates prompt engineering by defining a clear and specific prompt template that guides the LLM to generate the desired summary. The prompt includes instructions on style, length, mathematical details, and the use of analogies.\n",
      "*   **LangChain Abstraction:** The code leverages LangChain to abstract away the complexities of interacting with the LLM.  The `PromptTemplate` and `chain` abstractions simplify the process of creating and executing LLM workflows.\n",
      "*   **Streamlit Interface:** The code uses Streamlit to create a user-friendly interface for the summarization tool.  Streamlit allows users to easily select the research paper, explanation style, and length.\n",
      "*   **Template Loading/Saving:** The code demonstrates how to load and save prompt templates to external files, which can be useful for managing and versioning prompts.\n",
      "*   **Chaining:** The `template | llm` syntax is a concise way to create a LangChain chain that combines the prompt template and the LLM. This pattern is common in LangChain applications.\n",
      "\n",
      "In summary, the repository provides a practical example of how to use LangChain and a large language model (Gemini) to build a research paper summarization tool with a customizable user interface. The code showcases best practices for prompt engineering, environment variable management, and user interface design.\n",
      "\n",
      "üìÅ Source Files:\n",
      "- promptgenerator.py\n",
      "- Paper_Summarizer.py\n",
      "- requirements.txt\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Gemini Analysis:\n",
      "Okay, let's analyze the provided code and discuss potential improvements, focusing on technical aspects and best practices.\n",
      "\n",
      "**Overall Structure and Functionality**\n",
      "\n",
      "The code implements a Streamlit application that leverages Langchain and Google Gemini (via `langchain-google-genai`) to summarize research papers.  It takes user input for the paper title, explanation style, and length, then uses a pre-defined prompt template to generate a summary using the Gemini model.\n",
      "\n",
      "**Areas for Improvement and Detailed Suggestions**\n",
      "\n",
      "1. **Error Handling and Input Validation:**\n",
      "\n",
      "   *   **API Key Check:**  The code relies on the `GEMINI_API_KEY` environment variable.  It should include a check to ensure this variable is set before attempting to initialize the `ChatGoogleGenerativeAI` model.  If the key is missing, display a user-friendly error message in the Streamlit app.\n",
      "\n",
      "     ```python\n",
      "     if not api_key:\n",
      "         st.error(\"GEMINI_API_KEY environment variable not set.  Please set it and restart the app.\")\n",
      "         st.stop()  # Halt execution if the API key is missing\n",
      "     ```\n",
      "\n",
      "   *   **Model Availability:**  The `gemini-2.0-flash` model might not always be available or have rate limits.  Implement error handling to gracefully catch exceptions raised by the Gemini API (e.g., `GoogleGenerativeAIError`) and display informative messages to the user.  Consider allowing the user to select a different Gemini model as a fallback.\n",
      "\n",
      "     ```python\n",
      "     try:\n",
      "         llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=api_key)\n",
      "     except Exception as e:\n",
      "         st.error(f\"Error initializing Gemini model: {e}\")\n",
      "         st.stop()\n",
      "     ```\n",
      "\n",
      "   *   **Empty Input Handling:** Although `st.selectbox` prevents completely empty selections, consider what happens if the Gemini model *still* returns an empty or error response. Add a check after `result = chain.invoke(...)` to handle empty or error responses gracefully.\n",
      "\n",
      "     ```python\n",
      "     if st.button('Summarize'):\n",
      "         chain = template | llm\n",
      "         try:\n",
      "             result = chain.invoke({\n",
      "                 'paper_input':paper_input,\n",
      "                 'style_input':style_input,\n",
      "                 'length_input':length_input\n",
      "             })\n",
      "             if result and result.content:  # Check if result and content exist\n",
      "                 st.write(result.content)\n",
      "             else:\n",
      "                 st.warning(\"Gemini returned an empty or invalid response.  Please try again or adjust your input.\")\n",
      "         except Exception as e:\n",
      "             st.error(f\"Error during summarization: {e}\")\n",
      "     ```\n",
      "\n",
      "2. **Asynchronous Execution (Optional but Recommended for Responsiveness):**\n",
      "\n",
      "   *   The `chain.invoke()` call can take time, especially for longer papers or complex summaries.  Consider using `chain.ainvoke()` (the asynchronous version) in conjunction with `asyncio` to prevent the Streamlit app from blocking while waiting for the Gemini model to respond.  This will improve the user experience.  Streamlit has built-in support for asynchronous operations.\n",
      "\n",
      "     ```python\n",
      "     import asyncio\n",
      "\n",
      "     async def summarize(paper_input, style_input, length_input):\n",
      "         chain = template | llm\n",
      "         result = await chain.ainvoke({\n",
      "             'paper_input': paper_input,\n",
      "             'style_input': style_input,\n",
      "             'length_input': length_input\n",
      "         })\n",
      "         return result.content\n",
      "\n",
      "     if st.button('Summarize'):\n",
      "         with st.spinner(\"Summarizing...\"):  # Use a spinner to indicate loading\n",
      "             try:\n",
      "                 summary = asyncio.run(summarize(paper_input, style_input, length_input))\n",
      "                 st.write(summary)\n",
      "             except Exception as e:\n",
      "                 st.error(f\"Error during summarization: {e}\")\n",
      "     ```\n",
      "\n",
      "3. **Prompt Engineering and Template Flexibility:**\n",
      "\n",
      "   *   **Prompt Optimization:**  Experiment with different prompt templates to see what yields the best results.  The current prompt is a good starting point, but you can refine it to be more specific about the desired output format, tone, or focus.  Consider adding examples to the prompt (few-shot learning).\n",
      "   *   **Dynamic Prompt Construction:**  Instead of a single, static `template.json`, consider dynamically constructing the prompt based on the user's input.  This allows for more flexibility and customization.  You can use conditional logic within the prompt template to include or exclude certain sections based on the selected explanation style or length.\n",
      "   * **Prompt Injection Prevention:**  While the current prompt is relatively safe, always be mindful of prompt injection attacks.  If you allow users to input parts of the prompt directly (e.g., adding custom instructions), sanitize the input to prevent malicious code from being injected.\n",
      "\n",
      "4. **Caching (Important for Performance and Cost):**\n",
      "\n",
      "   *   **Caching Results:**  The Gemini API calls can be expensive and time-consuming. Implement caching to store the results of previous summarizations. Streamlit provides a simple caching mechanism using `@st.cache_data`.  Cache based on the paper title, style, and length inputs.\n",
      "\n",
      "     ```python\n",
      "     @st.cache_data\n",
      "     def generate_summary(paper_input, style_input, length_input):\n",
      "         chain = template | llm\n",
      "         result = chain.invoke({\n",
      "             'paper_input': paper_input,\n",
      "             'style_input': style_input,\n",
      "             'length_input': length_input\n",
      "         })\n",
      "         return result.content\n",
      "\n",
      "     if st.button('Summarize'):\n",
      "         with st.spinner(\"Summarizing...\"):\n",
      "             try:\n",
      "                 summary = generate_summary(paper_input, style_input, length_input)\n",
      "                 st.write(summary)\n",
      "             except Exception as e:\n",
      "                 st.error(f\"Error during summarization: {e}\")\n",
      "     ```\n",
      "\n",
      "5. **Code Organization and Modularity:**\n",
      "\n",
      "   *   **Function Decomposition:** Break down the code into smaller, more manageable functions.  For example, create separate functions for loading the prompt, initializing the Gemini model, generating the summary, and displaying the results.\n",
      "   *   **Configuration Management:**  Consider using a configuration file (e.g., `config.yaml` or `config.json`) to store settings like the Gemini model name, API key location, and other parameters.  This makes it easier to manage and modify the application's configuration without changing the code.\n",
      "\n",
      "6. **User Interface (UI) Improvements:**\n",
      "\n",
      "   *   **Loading Indicator:** Use `st.spinner` or a similar loading indicator to provide visual feedback to the user while the summarization is in progress (as shown in the asynchronous example).\n",
      "   *   **Clearer Instructions:**  Provide clear instructions to the user on how to use the application.\n",
      "   *   **Paper Content Display (Optional):**  If possible, display the content of the selected research paper alongside the summary.  This allows the user to compare the summary to the original text.  You would need to find a way to access the paper content (e.g., by scraping it from a URL or using a PDF parsing library).\n",
      "   *   **Downloadable Summary:**  Add a button to allow the user to download the generated summary as a text file.\n",
      "\n",
      "7. **Template Loading:**\n",
      "\n",
      "   *   You load the template from `template.json` *after* defining the `style_input`, `length_input`, and `paper_input` `st.selectbox` widgets. This isn't necessarily wrong, but it could be more efficient to load the template once at the beginning of the script.\n",
      "\n",
      "8. **Dependency Management:**\n",
      "\n",
      "   *   Use a `requirements.txt` file to specify the project's dependencies. This makes it easier for others to reproduce your environment.\n",
      "\n",
      "**Example `requirements.txt`:**\n",
      "\n",
      "```\n",
      "langchain\n",
      "langchain-core\n",
      "langchain-openai\n",
      "langchain-anthropic\n",
      "langchain-google-genai\n",
      "openai\n",
      "google-generativeai\n",
      "transformers\n",
      "huggingface-hub\n",
      "python-dotenv\n",
      "numpy\n",
      "scikit-learn\n",
      "streamlit\n",
      "```\n",
      "\n",
      "**Revised Code Snippets (Illustrative)**\n",
      "\n",
      "```python\n",
      "import os\n",
      "import streamlit as st\n",
      "from langchain_core.prompts import PromptTemplate, load_prompt\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "from dotenv import load_dotenv\n",
      "import asyncio\n",
      "\n",
      "# Load environment variables\n",
      "load_dotenv()\n",
      "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
      "\n",
      "# --- Error Handling and Configuration ---\n",
      "if not api_key:\n",
      "    st.error(\"GEMINI_API_KEY environment variable not set. Please set it and restart the app.\")\n",
      "    st.stop()\n",
      "\n",
      "try:\n",
      "    llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=api_key)\n",
      "except Exception as e:\n",
      "    st.error(f\"Error initializing Gemini model: {e}\")\n",
      "    st.stop()\n",
      "\n",
      "# --- Load Prompt Template ---\n",
      "try:\n",
      "    template = load_prompt('template.json')\n",
      "except FileNotFoundError:\n",
      "    st.error(\"template.json not found.\")\n",
      "    st.stop()\n",
      "except Exception as e:\n",
      "    st.error(f\"Error loading template: {e}\")\n",
      "    st.stop()\n",
      "\n",
      "\n",
      "# --- Streamlit UI ---\n",
      "st.header('Research Tool')\n",
      "\n",
      "paper_input = st.selectbox(\"Select Research Paper Name\", [\"Attention Is All You Need\", \"BERT: Pre-training of Deep Bidirectional Transformers\", \"GPT-3: Language Models are Few-Shot Learners\", \"Diffusion Models Beat GANs on Image Synthesis\"])\n",
      "style_input = st.selectbox(\"Select Explanation Style\", [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"])\n",
      "length_input = st.selectbox(\"Select Explanation Length\", [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"])\n",
      "\n",
      "\n",
      "# --- Summarization Function (Asynchronous with Caching) ---\n",
      "@st.cache_data\n",
      "async def generate_summary(paper_input, style_input, length_input):\n",
      "    chain = template | llm\n",
      "    try:\n",
      "        result = await chain.ainvoke({\n",
      "            'paper_input': paper_input,\n",
      "            'style_input': style_input,\n",
      "            'length_input': length_input\n",
      "        })\n",
      "        if result and result.content:\n",
      "            return result.content\n",
      "        else:\n",
      "            return \"Gemini returned an empty or invalid response.\"\n",
      "    except Exception as e:\n",
      "        return f\"Error during summarization: {e}\"\n",
      "\n",
      "\n",
      "# --- Summarize Button ---\n",
      "if st.button('Summarize'):\n",
      "    with st.spinner(\"Summarizing...\"):\n",
      "        try:\n",
      "            summary = await generate_summary(paper_input, style_input, length_input) # Await the async function\n",
      "            st.write(summary)\n",
      "        except Exception as e:\n",
      "            st.error(f\"An unexpected error occurred: {e}\")\n",
      "```\n",
      "\n",
      "**Key Takeaways**\n",
      "\n",
      "*   Prioritize error handling and input validation to create a robust application.\n",
      "*   Consider asynchronous execution for a more responsive UI.\n",
      "*   Experiment with prompt engineering to optimize the quality of the summaries.\n",
      "*   Implement caching to reduce API costs and improve performance.\n",
      "*   Organize the code into smaller, more manageable functions.\n",
      "*   Enhance the UI to provide a better user experience.\n",
      "*   Use `requirements.txt` for dependency management.\n",
      "\n",
      "By implementing these improvements, you can create a more reliable, efficient, and user-friendly research summarization tool. Remember to test your changes thoroughly after each modification.\n",
      "\n",
      "üìÅ Source Files:\n",
      "- promptgenerator.py\n",
      "- Paper_Summarizer.py\n",
      "- requirements.txt\n",
      "--------------------------------------------------\n",
      "\n",
      "üëã Exiting chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "        user_query = input(\"You: \")\n",
    "        if user_query.lower() == \"exit\":\n",
    "            print(\"üëã Exiting chat. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Retrieve relevant code context\n",
    "        results = query_chroma(user_query)\n",
    "        relevant_chunks = results['documents'][0]\n",
    "        sources = results['metadatas'][0]\n",
    "\n",
    "        if not relevant_chunks:\n",
    "            print(\"‚ùå No relevant code found in repository\")\n",
    "            continue\n",
    "\n",
    "        # Generate LLM response\n",
    "        context = \"\\n\\n\".join(relevant_chunks)\n",
    "        prompt = f\"\"\"Analyze the following code context from a GitHub repository and answer the user's question.\n",
    "        Focus on technical implementation details and patterns found in the codebase.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {user_query}\n",
    "\n",
    "        Provide a detailed answer with code references where applicable.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = gemini.invoke(prompt)\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nü§ñ Gemini Analysis:\\n{response.content}\")\n",
    "        print(\"\\nüìÅ Source Files:\")\n",
    "        for src in {s['file'] for s in sources}:\n",
    "            print(f\"- {src}\")\n",
    "        print(\"-\" * 50 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
